{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/JuanManiglia/Guia_Tensorflow_Keras/Clase_06/clase_06_2_cnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YDTXd8-Lmp8Q"
   },
   "source": [
    "**Modulo 6 : Aplicaciones de las Redes Neuronales Profundas**\n",
    "* Instructor: [Juan Maniglia](https://juanmaniglia.github.io)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jf_otSJdmp8k"
   },
   "source": [
    "# Part 6.2: Redes neuronales Keras para dígitos y moda MNIST\n",
    "\n",
    "# Visión por computador\n",
    "\n",
    "Este módulo se centrará en la visión artificial. Existen algunas diferencias y similitudes importantes con las redes neuronales anteriores.\n",
    "\n",
    "* Usualmente usaremos clasificación, aunque la regresión sigue siendo una opción.\n",
    "* La entrada a la red neuronal ahora es 3D (alto, ancho, color)\n",
    "* Los datos no se transforman, no hay puntuaciones z ni variables ficticias.\n",
    "* El tiempo de procesamiento es mucho más largo.\n",
    "* Ahora tenemos diferentes tiempos de capa: capas densas (como antes), capas de convolución y capas de agrupación máxima.\n",
    "* Los datos ya no llegarán como archivos CSV. TensorFlow proporciona algunas utilidades para pasar directamente de la imagen a la entrada de una red neuronal.\n",
    "\n",
    "\n",
    "# Conjuntos de datos de visión artificial\n",
    "\n",
    "Hay muchos conjuntos de datos para la visión artificial. Dos de los conjuntos de datos clásicos más populares son el conjunto de datos de dígitos MNIST y los conjuntos de datos de imágenes CIFAR. No usaremos ninguno de estos conjuntos de datos en este curso, pero es importante estar familiarizado con ellos, ya que los textos sobre redes neuronales a menudo se refieren a ellos.\n",
    "\n",
    "El [MNIST Digits Data Set](http://yann.lecun.com/exdb/mnist/) es muy popular en la comunidad de investigación de redes neuronales. Una muestra de ello se puede ver en la Figura 6.MNIST.\n",
    "\n",
    "**Figure 6.MNIST: MNIST Data Set**\n",
    "![MNIST Data Set](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/class_8_mnist.png \"MNIST Data Set\")\n",
    "\n",
    "[Fashion-MNIST](https://www.kaggle.com/zalando-research/fashionmnist) es un dataset de [Zalando](https://jobs.zalando.com/tech/) imágenes de artículos, que consisten en un conjunto de entrenamiento de 60 000 ejemplos y un conjunto de prueba de 10 000 ejemplos. Cada ejemplo es una imagen en escala de grises de 28x28, asociada con una etiqueta de 10 clases. Fashion-MNIST está destinado a servir como un **reemplazo inmediato** directo para el original [MNIST dataset](http://yann.lecun.com/exdb/mnist/)para comparar algoritmos de aprendizaje automático. Comparte el mismo tamaño de imagen y estructura de las divisiones de entrenamiento y prueba. Estos datos se pueden ver en la Figura 6.MNIST-FASHION.\n",
    "\n",
    "**Figure 6.MNIST-FASHION: MNIST Fashon Data Set**\n",
    "![mnist-fashion](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/mnist-fashion.png \"mnist-fashion\")\n",
    "\n",
    "El [CIFAR-10 and CIFAR-100](https://www.cs.toronto.edu/~kriz/cifar.html) datasets también es utilizado con frecuencia por la comunidad de investigación de redes neuronales.\n",
    "\n",
    "**Figure 6.CIFAR: CIFAR Data Set**\n",
    "![CIFAR Data Set](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/class_8_cifar.png \"CIFAR Data Set\")\n",
    "\n",
    "El conjunto de datos CIFAR-10 contiene imágenes de baja resolución que se dividen en 10 clases. El conjunto de datos CIFAR-100 contiene 100 clases en una jerarquía.\n",
    "\n",
    "# Redes neuronales convolucionales (CNN)\n",
    "\n",
    "La red neuronal convolucional (CNN) es una tecnología de red neuronal que ha impactado profundamente en el área de la visión por computadora (CV). Fukushima (1980) [[Cite:fukushima1980neocognitron]](https://www.rctn.org/bruno/public/papers/Fukushima1980.pdf) introdujo el concepto original de una red neuronal convolucional, y LeCun, Bottou, Bengio & Haffner (1998) [[Cite:lecun1995convolutional]](http://yann.lecun.com/exdb/publis/pdf/lecun-bengio-95a.pdf) mejorado mucho este trabajo. A partir de esta investigación, Yan LeCun presentó la famosa arquitectura de red neuronal LeNet-5. Este capítulo sigue el estilo LeNet-5 de red neuronal convolucional.\n",
    "Aunque la visión por computadora utiliza principalmente CNN, esta tecnología tiene alguna aplicación fuera del campo. Debe darse cuenta de que si desea utilizar CNN en datos no visuales, debe encontrar una manera de codificar sus datos para que puedan imitar las propiedades de los datos visuales.\n",
    "\n",
    "Las CNN son algo similares a la arquitectura de mapas autoorganizados (SOM) que examinamos en el Capítulo 2, \"Mapas autoorganizados\". El orden de los elementos del vector es crucial para el entrenamiento. Por el contrario, la mayoría de las redes neuronales que no son CNN ni SOM tratan sus datos de entrada como un vector largo de valores, y el orden en que organiza las características entrantes en este vector es irrelevante. Para estos tipos de redes neuronales, no puede cambiar el orden después de haber entrenado la red. En otras palabras, las CNN y los SOM no siguen el tratamiento estándar de los vectores de entrada.\n",
    "La red SOM dispuso las entradas en una cuadrícula. Esta disposición funcionó bien con las imágenes porque los píxeles más cercanos entre sí son importantes entre sí. Obviamente, el orden de los píxeles en una imagen es significativo. El cuerpo humano es un ejemplo relevante de este tipo de orden. Para el diseño del rostro, estamos acostumbrados a que los ojos estén cerca uno del otro. De la misma manera, los tipos de redes neuronales como los SOM se adhieren a un orden de píxeles. En consecuencia, tienen muchas aplicaciones para la visión artificial.\n",
    "\n",
    "Este avance en las CNN se debe a años de investigación sobre ojos biológicos. En otras palabras, las CNN utilizan campos de entrada superpuestos para simular características de ojos biológicos. Hasta este avance, la IA no había podido reproducir las capacidades de la visión biológica.\n",
    "La escala, la rotación y el ruido han presentado desafíos en el pasado para la investigación de la visión por computadora con IA. Puede observar la complejidad de los ojos biológicos en el siguiente ejemplo. Un amigo levanta una hoja de papel que tiene escrito un gran número. A medida que su amigo se acerca a usted, el número sigue siendo identificable. De la misma manera, aún puedes identificar el número cuando tu amigo gira el papel. Por último, tu amigo crea ruido dibujando líneas en la parte superior de la página, pero aún puedes identificar el número. Como puede ver, estos ejemplos demuestran la alta función del ojo biológico y le permiten comprender mejor el avance de la investigación de las CNN. Es decir, esta red neuronal tiene la capacidad de procesar escala, rotación y ruido en el campo de la visión artificial. Esta estructura de red se puede ver en la Figura 6.LENET.\n",
    "\n",
    "**Figure 6.LENET: A LeNET-5 Network (LeCun, 1998)**\n",
    "![A LeNET-5 Network](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/class_8_lenet5.png \"A LeNET-5 Network\")\n",
    "\n",
    "Hasta ahora solo hemos visto un tipo de capa (capas densas). Al final de este curso habremos visto:\n",
    "\n",
    "* **Dense Layers**: capas completamente conectadas. (presentado anteriormente)\n",
    "* **Convolution Layers**: se utilizan para escanear imágenes. (presentó esta clase)\n",
    "* **Max Pooling Layers**: se utiliza para reducir la resolución de las imágenes. (presentó esta clase)\n",
    "* **Dropout Layer**: se utiliza para agregar regularización. (presentado en la siguiente clase)\n",
    "\n",
    "## Capas de convolución\n",
    "\n",
    "La primera capa que examinaremos es la capa convolucional. Comenzaremos observando los hiperparámetros que debe especificar para una capa convolucional en la mayoría de los marcos de trabajo de redes neuronales que admiten la CNN:\n",
    "\n",
    "* Número de filtros\n",
    "* Tamaño del filtro\n",
    "* zancada\n",
    "* Relleno\n",
    "* Función de activación/no linealidad\n",
    "\n",
    "El objetivo principal de una capa convolucional es detectar características como bordes, líneas, manchas de color y otros elementos visuales. Los filtros pueden detectar estas características. Cuantos más filtros le damos a una capa convolucional, más características puede detectar.\n",
    "\n",
    "Un filtro es un objeto de forma cuadrada que escanea la imagen. Una cuadrícula puede representar los píxeles individuales de una cuadrícula. Puede pensar en la capa convolucional como una cuadrícula más pequeña que se desplaza de izquierda a derecha sobre cada fila de la imagen. También hay un hiperparámetro que especifica tanto el ancho como la altura del filtro de forma cuadrada. La siguiente figura muestra esta configuración en la que se ven los seis filtros convolucionales barriendo la cuadrícula de la imagen:\n",
    "\n",
    "Una capa convolucional tiene pesos entre ella y la capa o cuadrícula de imagen anterior. Cada píxel en cada capa convolucional es un peso. Por tanto, el número de pesos entre una capa convolucional y su capa predecesora o campo de imagen es el siguiente:\n",
    "\n",
    "```\n",
    "[FilterSize] * [FilterSize] * [# of Filters]\n",
    "```\n",
    "\n",
    "Por ejemplo, si el tamaño del filtro fuera 5 (5x5) para 10 filtros, habría 250 pesos.\n",
    "\n",
    "Debe comprender cómo los filtros convolucionales barren la salida o la cuadrícula de imagen de la capa anterior. La Figura 6.CNN ilustra el barrido:\n",
    "\n",
    "**Figure 6.CNN: Convolutional Neural Network**\n",
    "![Convolutional Neural Network](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/class_8_cnn_grid.png \"Convolutional Neural Network\")\n",
    "\n",
    "La figura anterior muestra un filtro convolucional con un tamaño de 4 y un tamaño de relleno de 1. El tamaño de relleno es responsable del borde de ceros en el área que barre el filtro. Aunque la imagen es en realidad de 8x7, el relleno adicional proporciona un tamaño de imagen virtual de 9x8 para que el filtro se desplace. El paso especifica el número de posiciones en las que se detendrán los filtros convolucionales. Los filtros convolucionales se mueven hacia la derecha, avanzando el número de celdas especificado en el paso. Una vez que se alcanza el extremo derecho, el filtro convolucional vuelve al extremo izquierdo, luego se mueve hacia abajo por la cantidad de zancada y\n",
    "continúa hacia la derecha de nuevo.\n",
    "\n",
    "Existen algunas limitaciones en relación con el tamaño de la zancada. Obviamente, la zancada no puede ser 0. El filtro convolucional nunca se movería si la zancada se estableciera en 0. Además, ni la zancada ni el tamaño del filtro convolucional pueden ser mayores que la cuadrícula anterior. Existen restricciones adicionales en el paso (s), el relleno (p) y el ancho del filtro (f) para una imagen de ancho (w). Específicamente, el filtro convolucional debe poder comenzar en el extremo izquierdo o borde superior, moverse un cierto número de pasos y aterrizar en el extremo derecho o borde inferior. La siguiente ecuación muestra el número de pasos que un operador convolucional\n",
    "debe tomar para cruzar la imagen:\n",
    "\n",
    "$$ steps = \\frac{w - f + 2p}{s+1} $$\n",
    "\n",
    "El número de pasos debe ser un número entero. En otras palabras, no puede tener decimales. El propósito del relleno (p) es ajustarse para que esta ecuación se convierta en un valor entero.\n",
    "\n",
    "## Max Pooling Layers\n",
    "\n",
    "Max-pool layers reduce la muestra de un cuadro 3D a uno nuevo con dimensiones más pequeñas. Por lo general, siempre puede colocar un max-pool layer inmediatamente después de la capa convolucional. El LENET muestra la max-pool layerinmediatamente después de las capas C1 y C3. Estos max-pool layers disminuir progresivamente el tamaño de las dimensiones de las cajas 3D que las atraviesan. Esta técnica puede evitar el sobreajuste (Krizhevsky, Sutskever & Hinton, 2012).\n",
    "\n",
    "Una capa de agrupación tiene los siguientes hiperparámetros:\n",
    "\n",
    "* Extensión espacial (f)\n",
    "* Paso(s)\n",
    "\n",
    "A diferencia de las capas convolucionales, max-pool layers no usa relleno. Adicionalmente, max-pool layers no tienen pesas, por lo que el entrenamiento no les afecta. Estas capas simplemente reducen la muestra de su entrada de cuadro 3D. La salida de la caja 3D por un max-pool layer tendrá un ancho igual a esta ecuación:\n",
    "\n",
    "$$ w_2 = \\frac{w_1 - f}{s + 1} $$\n",
    "\n",
    "La altura de la caja 3D producida por el max-pool layer se calcula de manera similar con esta ecuación:\n",
    "\n",
    "$$ h_2 = \\frac{h_1 - f}{s + 1} $$\n",
    "\n",
    "La profundidad de la caja 3D producida por el max-pool layer es igual a la profundidad que el cuadro 3D recibió como entrada. La configuración más común para los hiperparámetros de un max-pool layer son f=2 y s=2. La extensión espacial (f) especifica que los cuadros de 2x2 se reducirán a píxeles individuales. De estos cuatro píxeles, el píxel con el valor máximo representará el píxel 2x2 en la nueva cuadrícula. Debido a que los cuadrados de tamaño 4 se reemplazan con el tamaño 1, se pierde el 75% de la información de píxeles. La siguiente figura muestra esta transformación cuando una cuadrícula de 6x6 se convierte en 3x3:\n",
    "\n",
    "**Figure 6.MAXPOOL: Max Pooling Layer**\n",
    "![Max Pooling Layer](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/class_8_conv_maxpool.png \"Max Pooling Layer\")\n",
    "\n",
    "Por supuesto, el diagrama anterior muestra cada píxel como un solo número. Una imagen en escala de grises tendría esta característica. Para una imagen RGB, generalmente tomamos el promedio de los tres números para determinar qué píxel tiene el valor máximo.\n",
    "\n",
    "# Redes neuronales convolucionales de regresión\n",
    "\n",
    "Ahora veremos dos ejemplos, uno para regresión y otro para clasificación. Para la visión artificial supervisada, su conjunto de datos necesitará algunas etiquetas. Para la clasificación, esta etiqueta suele especificar de qué se trata la imagen. Para la regresión, esta \"etiqueta\" es una cantidad numérica que debe producir la imagen, como un conteo. Veremos dos medios diferentes de proporcionar esta etiqueta.\n",
    "\n",
    "El primer ejemplo mostrará cómo manejar la regresión con redes neuronales de convolución. En este caso, proporcionaremos una imagen y esperaremos que la red neuronal cuente los elementos de esa imagen. Usaremos un [dataset](https://www.kaggle.com/jeffheaton/count-the-paperclips) que creé que contiene un número aleatorio de clips. El siguiente código descargará este conjunto de datos por usted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    %tensorflow_version 2.x\n",
    "    COLAB = True\n",
    "    print(\"Note: using Google CoLab\")\n",
    "except:\n",
    "    print(\"Note: not using Google CoLab\")\n",
    "    COLAB = False\n",
    "\n",
    "# Nicely formatted time string\n",
    "def hms_string(sec_elapsed):\n",
    "    h = int(sec_elapsed / (60 * 60))\n",
    "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
    "    s = sec_elapsed % 60\n",
    "    return f\"{h}:{m:>02}:{s:>05.2f}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GpfadrdQcVg8"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import wget\n",
    "from zipfile import ZipFile\n",
    "\n",
    "DOWNLOAD_SOURCE = \"https://github.com/jeffheaton/data-mirror/releases/download/v1/paperclips.zip\"\n",
    "DOWNLOAD_NAME = DOWNLOAD_SOURCE[DOWNLOAD_SOURCE.rfind('/')+1:]\n",
    "\n",
    "if COLAB:\n",
    "  PATH = \"/content\"\n",
    "else:\n",
    " \n",
    "  PATH = R\"C:\\Users\\jamr1\\temp\"\n",
    "\n",
    "EXTRACT_TARGET = os.path.join(PATH,\"clips\")\n",
    "SOURCE = os.path.join(EXTRACT_TARGET, \"paperclips\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ivCHAirHpNyT"
   },
   "source": [
    "A continuación, descargamos las imágenes. Esta parte depende del origen de tus imágenes. El siguiente código descarga imágenes de una URL, donde un archivo ZIP contiene las imágenes. El código descomprime el archivo ZIP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CExT2Z6gpAhz",
    "outputId": "30c455eb-03aa-4a46-e582-7fd9ffefcddc"
   },
   "outputs": [],
   "source": [
    "if COLAB:\n",
    "    !wget -O {os.path.join(PATH,DOWNLOAD_NAME)} {DOWNLOAD_SOURCE}\n",
    "    !mkdir -p {SOURCE}\n",
    "    !mkdir -p {TARGET}\n",
    "    !mkdir -p {EXTRACT_TARGET}\n",
    "    !unzip -o -j -d {SOURCE} {os.path.join(PATH, DOWNLOAD_NAME)} >/dev/null\n",
    "else:\n",
    "    wget.download(DOWNLOAD_SOURCE, out=PATH)\n",
    "    os.makedirs(SOURCE)\n",
    "    with ZipFile(PATH +'\\\\'+ DOWNLOAD_NAME, 'r') as f:\n",
    "        f.extractall(SOURCE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p4LWl8TzzI8a"
   },
   "source": [
    "Para la regresión, las etiquetas están contenidas en un archivo CSV denominado **train.csv**. Este archivo tiene solo dos etiquetas, **id** y **clip_count**. El ID especifica el nombre del archivo; por ejemplo, el ID de fila 1 corresponde al archivo **clips-1.jpg**. El siguiente código carga las etiquetas para el conjunto de entrenamiento y crea una nueva columna, llamada **nombre de archivo**, que contiene el nombre de archivo de cada imagen, según la columna **id**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w4W4LeGSqYya"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\n",
    "    os.path.join(SOURCE,\"train.csv\"), \n",
    "    na_values=['NA', '?'])\n",
    "\n",
    "df['filename']=\"clips-\"+df[\"id\"].astype(str)+\".jpg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AGxjITNtz0sC"
   },
   "source": [
    "Dataset Cargado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "rJpaMpmfswIp",
    "outputId": "0ec161da-7e27-4d9f-b4ee-26c40e236cc4"
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nN1AsQeysmGd"
   },
   "source": [
    "\n",
    "Separar en entrenamiento y validación (para early stopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CI4_qNaSqp31",
    "outputId": "9ba5822c-7f55-49ee-e7b1-a5b7d7433a78"
   },
   "outputs": [],
   "source": [
    "TRAIN_PCT = 0.8\n",
    "TRAIN_CUT = int(len(df) * TRAIN_PCT)\n",
    "\n",
    "df_train = df[0:TRAIN_CUT]\n",
    "df_validate = df[TRAIN_CUT:]\n",
    "\n",
    "print(f\"Training size: {len(df_train)}\")\n",
    "print(f\"Validate size: {len(df_validate)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KNoqk5uIz6FW"
   },
   "source": [
    "Ahora estamos listos para crear dos objetos ImageDataGenerator. Actualmente estamos usando un generador, que crea datos de entrenamiento adicionales mediante la manipulación del material de origen. Esta técnica puede producir redes neuronales considerablemente más fuertes. El generador a continuación voltea las imágenes tanto vertical como horizontalmente. Keras entrenará la red de neuronas tanto en las imágenes originales como en las imágenes invertidas. Este aumento aumenta considerablemente el tamaño de los datos de entrenamiento. El módulo 6.4 profundiza en las transformaciones que puede realizar. También puede especificar un tamaño objetivo para cambiar el tamaño de las imágenes automáticamente.\n",
    "\n",
    "La función **flow_from_dataframe** carga las etiquetas de un dataframe de Pandas conectado a nuestro archivo **train.csv**. Cuando demostremos la clasificación, usaremos el método flow_from_directory; que carga las etiquetas desde la estructura del directorio en lugar de un CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YZzeAHdfsy0O",
    "outputId": "7cb467b4-61e0-4ab8-95fa-fe79e83b4b16"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras_preprocessing\n",
    "from keras_preprocessing import image\n",
    "from keras_preprocessing.image import ImageDataGenerator\n",
    "\n",
    "training_datagen = ImageDataGenerator(\n",
    "  rescale = 1./255,\n",
    "  horizontal_flip=True,\n",
    "  vertical_flip=True,\n",
    "  fill_mode='nearest')\n",
    "\n",
    "train_generator = training_datagen.flow_from_dataframe(\n",
    "        dataframe=df_train,\n",
    "        directory=SOURCE,\n",
    "        x_col=\"filename\",\n",
    "        y_col=\"clip_count\",\n",
    "        target_size=(256, 256),\n",
    "        batch_size=32,\n",
    "        class_mode='other')\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "val_generator = validation_datagen.flow_from_dataframe(\n",
    "        dataframe=df_validate,\n",
    "        directory=SOURCE,\n",
    "        x_col=\"filename\",\n",
    "        y_col=\"clip_count\",\n",
    "        target_size=(256, 256),\n",
    "        class_mode='other')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_GVeYo6p2sdG"
   },
   "source": [
    "Ahora podemos entrenar la red neuronal. El código para construir y entrenar la red neuronal no es tan diferente al de los módulos anteriores. Haremos uso de la clase Keras Sequential para proporcionar capas a la red neuronal. Ahora tenemos varios tipos de capas nuevos que no habíamos visto anteriormente.\n",
    "\n",
    "* **Conv2D** - Las capas de convolución.\n",
    "* **MaxPooling2D** - Las capas máximas de agrupación.\n",
    "* **Flatten** - Aplane los tensores 2D (y superiores) para permitir que se procese una capa Densa.\n",
    "* **Dense** - Capas densas, las mismas que se demostraron anteriormente. Las capas densas a menudo forman las capas de salida final de la red neuronal.\n",
    "\n",
    "El código de entrenamiento es muy similar al anterior. Este código es para regresión, por lo que se usa una activación final lineal, junto con mean_squared_error para la función de pérdida. El generador proporciona las matrices *x* e *y* que proporcionamos anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DBHlqCIgtWSq",
    "outputId": "cba1cdf7-9f2f-4a33-8049-655a35e25ac6"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import time\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    # Tenga en cuenta que la forma de entrada es el tamaño deseado de la imagen 150x150 con color de 3 bytes\n",
    "    # Esta es la primera convolución\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(256, 256, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    # La segunda convolución\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    # 512 capas de neuronas ocultas\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='linear')\n",
    "])\n",
    "\n",
    "\n",
    "model.summary()\n",
    "epoch_steps = 250\n",
    "validation_steps = len(df_validate)\n",
    "model.compile(loss = 'mean_squared_error', optimizer='adam')\n",
    "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto',\n",
    "        restore_best_weights=True)\n",
    "\n",
    "start_time = time.time()\n",
    "history = model.fit(train_generator,  \n",
    "  verbose = 1,\n",
    "  validation_data=val_generator, callbacks=[monitor], epochs=25)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(\"Elapsed time: {}\".format(hms_string(elapsed_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gNiELOX53PtU"
   },
   "source": [
    "## Score de datos de imagen de regresión\n",
    "\n",
    "El score/predict de un generador es un poco diferente al entrenamiento. No queremos imágenes aumentadas, y no queremos que el conjunto de datos se mezcle. Para la puntuación, queremos una predicción para cada entrada. Construimos el generador de la siguiente manera:\n",
    "\n",
    "* shuffle=False\n",
    "* batch_size=1\n",
    "* class_mode=None\n",
    "\n",
    "Usamos un **batch_size** de 1 para garantizar que no nos quedemos sin memoria GPU si nuestro conjunto de predicciones es grande. Puede aumentar este valor para obtener un mejor rendimiento. **class_mode** es Ninguno porque no hay ***y*** o etiqueta. Después de todo, estamos prediciendo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QFXvtr-NtlQt",
    "outputId": "59ad62c8-667f-4717-a0ea-c1419f9a3b14"
   },
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\n",
    "    os.path.join(SOURCE,\"test.csv\"), \n",
    "    na_values=['NA', '?'])\n",
    "\n",
    "df_test['filename']=\"clips-\"+df_test[\"id\"].astype(str)+\".jpg\"\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "test_generator = validation_datagen.flow_from_dataframe(\n",
    "        dataframe=df_test,\n",
    "        directory=SOURCE,\n",
    "        x_col=\"filename\",\n",
    "        batch_size=1,\n",
    "        shuffle=False,\n",
    "        target_size=(256, 256),\n",
    "        class_mode=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TOuNIlpLAF5A"
   },
   "source": [
    "Necesitamos reiniciar el generador para asegurarnos de estar siempre al principio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gS0CjJ4bt8jQ"
   },
   "outputs": [],
   "source": [
    "test_generator.reset()\n",
    "pred = model.predict(test_generator,steps=len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4n-t8k5bt_nG"
   },
   "outputs": [],
   "source": [
    "df_submit = pd.DataFrame({'id':df_test['id'],'clip_count':pred.flatten()})\n",
    "df_submit.to_csv(os.path.join(PATH,\"submit.csv\"),index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sicFJd8u5c3v"
   },
   "source": [
    "# Clasificación de redes neuronales\n",
    "\n",
    "Al igual que antes en este módulo, cargaremos datos. Sin embargo, esta vez usaremos un conjunto de datos de imágenes de tres tipos diferentes de la flor del iris. Este archivo zip contiene tres directorios diferentes que especifican la etiqueta de cada imagen. Los directorios tienen el mismo nombre que las etiquetas:\n",
    "\n",
    "* iris-setosa\n",
    "* iris-versicolour\n",
    "* iris-virginica\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zxeLaa1c5gGA"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "URL = \"https://github.com/jeffheaton/data-mirror/releases\n",
    "DOWNLOAD_SOURCE = URL+\"/download/v1/iris-image.zip\"\n",
    "DOWNLOAD_NAME = DOWNLOAD_SOURCE[DOWNLOAD_SOURCE.rfind('/')+1:]\n",
    "\n",
    "if COLAB:\n",
    "  PATH = \"/content\"\n",
    "  EXTRACT_TARGET = os.path.join(PATH,\"iris\")\n",
    "  SOURCE = EXTRACT_TARGET\n",
    "else:\n",
    "  PATH = R\"C:\\Users\\jamr1\\temp\"\n",
    "  EXTRACT_TARGET = os.path.join(PATH,\"iris\")\n",
    "  SOURCE = EXTRACT_TARGET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hset2s3P9MFV"
   },
   "source": [
    "Just as before we unzip the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9sQdvl9F6Xru",
    "outputId": "0b653cea-3354-4d32-cc9f-1b55cda29e1d"
   },
   "outputs": [],
   "source": [
    "if COLAB:\n",
    "    !wget -O {os.path.join(PATH,DOWNLOAD_NAME)} {DOWNLOAD_SOURCE}\n",
    "    !mkdir -p {SOURCE}\n",
    "    !mkdir -p {TARGET}\n",
    "    !mkdir -p {EXTRACT_TARGET}\n",
    "    !unzip -o -d {EXTRACT_TARGET} {os.path.join(PATH, DOWNLOAD_NAME)} >/dev/null\n",
    "else:\n",
    "    wget.download(DOWNLOAD_SOURCE, out=PATH)\n",
    "    os.makedirs(SOURCE)\n",
    "    with ZipFile(PATH +'\\\\'+ DOWNLOAD_NAME, 'r') as f:\n",
    "        f.extractall(SOURCE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cYDFKA8i9KMP"
   },
   "source": [
    "Configuramos el generador, similar a antes. Esta vez usamos flow_from_directory para obtener las etiquetas de la estructura del directorio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u7EgpVqAdGvI",
    "outputId": "eb49248b-c7c3-4f1b-c1c8-f9d096b7eaed"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras_preprocessing\n",
    "from keras_preprocessing import image\n",
    "from keras_preprocessing.image import ImageDataGenerator\n",
    "\n",
    "training_datagen = ImageDataGenerator(\n",
    "  rescale = 1./255,\n",
    "  horizontal_flip=True,\n",
    "  vertical_flip=True,\n",
    "  width_shift_range=[-200,200],\n",
    "  #height_shift_range=0.5,\n",
    "  rotation_range=360,\n",
    "\n",
    "  fill_mode='nearest')\n",
    "\n",
    "train_generator = training_datagen.flow_from_directory(\n",
    "    directory=SOURCE, target_size=(256, 256), \n",
    "    class_mode='categorical', batch_size=8, shuffle=True)\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    directory=SOURCE, target_size=(256, 256), \n",
    "    class_mode='categorical', batch_size=8, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AFesQtNZBZP0"
   },
   "source": [
    "Entrenar la red neuronal con clasificación es similar a la regresión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MBnwiM-XflQc",
    "outputId": "be0fc753-952b-480c-be7e-b00e3a8303c7"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    # Tenga en cuenta que la forma de entrada es el tamaño deseado de la imagen 300x300 con 3 bytes de color\n",
    "    # Esta es la primera convolución\n",
    "    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(256, 256, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    # La segunda convolución\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    # La tercera convolución\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    # La cuarta convolución\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    # La quinta convolución\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    # Aplanar los resultados para alimentar a un DNN\n",
    "    \n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    # Capa oculta de 512 neuronas\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy', optimizer='adam')\n",
    "\n",
    "history = model.fit(train_generator, epochs=15, steps_per_epoch=10, \n",
    "                    verbose = 1)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of t81_558_class_06_2_cnn.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "129c28eb7371f81f3b772b69408a02d1e141a0d5c957c78df09fa0d1b2bc4f41"
  },
  "kernelspec": {
   "display_name": "Python 3.9 (tensorflow)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
