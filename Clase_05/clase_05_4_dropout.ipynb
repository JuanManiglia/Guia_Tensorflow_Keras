{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modulo 5 : Regularización y Dropout**\n",
    "* Instructor: [Juan Maniglia](https://juanmaniglia.github.io)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5.4: Drop Out para que Keras disminuya _Overfitting_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hinton, Srivastava, Krizhevsky, Sutskever y Salakhutdinov (2012) introdujeron el algoritmo de regularización de la deserción. Aunque el abandono funciona de manera diferente que L1 y L2, logra el mismo objetivo: la prevención del sobreajuste. Sin embargo, el algoritmo realiza la tarea eliminando neuronas y conexiones, al menos temporalmente. A diferencia de L1 y L2, no se agrega penalización de peso. Dropout no busca directamente entrenar pesos pequeños.\n",
    "El abandono funciona haciendo que las neuronas ocultas de la red neuronal no estén disponibles durante parte del entrenamiento. La eliminación de parte de la red neuronal hace que la parte restante se entrene para lograr una buena puntuación incluso sin las neuronas eliminadas. Esto disminuye la coadaptación entre neuronas, lo que resulta en menos sobreajuste.\n",
    "\n",
    "La mayoría de los marcos de redes neuronales implementan el abandono como una capa separada. Las capas de abandono funcionan como una capa de red neuronal regular densamente conectada. La única diferencia es que las capas de abandono perderán periódicamente algunas de sus neuronas durante el entrenamiento. Puede usar capas de abandono en redes neuronales de alimentación directa regulares.\n",
    "\n",
    "El programa implementa una capa de abandono como una capa densa que puede eliminar algunas de sus neuronas. Contrariamente a la creencia popular sobre la capa de abandono, el programa no elimina de forma permanente estas neuronas desechadas. Una capa de abandono no pierde ninguna de sus neuronas durante el proceso de entrenamiento y seguirá teniendo exactamente la misma cantidad de neuronas después del entrenamiento. De esta manera, el programa solo enmascara temporalmente las neuronas en lugar de descartarlas.\n",
    "La Figura 5. DROPOUT muestra cómo una capa de abandono podría ubicarse con otras capas.\n",
    "\n",
    "**Figure 5.DROPOUT: Dropout Regularization**\n",
    "![Dropout Regularization](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/class_9_dropout.png \"Dropout Regularization\")\n",
    "\n",
    "Las neuronas descartadas y sus conexiones se muestran como líneas discontinuas. La capa de entrada tiene dos neuronas de entrada, así como una neurona de polarización. La segunda capa es una capa densa con tres neuronas y una neurona de polarización. La tercera capa es una capa de abandono con seis neuronas regulares a pesar de que el programa ha eliminado el 50% de ellas. Si bien el programa elimina estas neuronas, no las calcula ni las entrena. Sin embargo, la red neuronal final utilizará todas estas neuronas para la salida. Como se mencionó anteriormente, el programa solo descarta temporalmente las neuronas.\n",
    "\n",
    "Durante las iteraciones de entrenamiento posteriores, el programa elige diferentes conjuntos de neuronas de la capa de abandono. Aunque elegimos una probabilidad de abandono del 50%, la computadora no necesariamente dejará caer tres neuronas. Es como si lanzáramos una moneda al aire para cada una de las neuronas candidatas a abandonar para elegir si esa neurona se abandonó. Debe saber que el programa nunca debe dejar caer la neurona de polarización. Solo las neuronas regulares en una capa de abandono son candidatas.\n",
    "La implementación del algoritmo de entrenamiento influye en el proceso de descarte de neuronas. El conjunto de abandonos cambia con frecuencia una vez por iteración o lote de entrenamiento. El programa también puede proporcionar intervalos donde todas las neuronas están presentes. Algunos marcos de redes neuronales brindan hiperparámetros adicionales para permitirle especificar exactamente la tasa de este intervalo.\n",
    "\n",
    "Por qué la deserción es capaz de disminuir el sobreajuste es una pregunta común. La respuesta es que la deserción puede reducir la posibilidad de que se desarrolle una codependencia entre dos neuronas. Dos neuronas que desarrollan una codependencia no podrán operar de manera efectiva cuando una se abandona. Como resultado, la red neuronal ya no puede depender de la presencia de cada neurona y se entrena en consecuencia. Esta característica disminuye su capacidad para memorizar la información que se le presenta, lo que obliga a la generalización.\n",
    "\n",
    "La deserción también disminuye el sobreajuste al forzar un proceso de arranque en la red neuronal. Bootstrapping es una técnica de conjunto muy común. Discutiremos el ensamblaje con mayor detalle en el Capítulo 16, “Modelado con redes neuronales”. Básicamente, el ensamblaje es una técnica de aprendizaje automático que combina múltiples modelos para producir un mejor resultado que los logrados por modelos individuales. Conjunto es un término que se origina en los conjuntos musicales en los que el producto musical final que escucha el público es la combinación de muchos instrumentos.\n",
    "\n",
    "Bootstrapping es una de las técnicas de conjunto más simples. El programador que usa bootstrapping simplemente entrena una serie de redes neuronales para realizar exactamente la misma tarea. Sin embargo, cada una de estas redes neuronales funcionará de manera diferente debido a algunas técnicas de entrenamiento y los números aleatorios utilizados en la inicialización del peso de la red neuronal. La diferencia de pesos provoca la variación del rendimiento. La salida de este conjunto de redes neuronales se convierte en la salida promedio de los miembros tomados en conjunto. Este proceso disminuye el sobreajuste a través del consenso de redes neuronales entrenadas de manera diferente.\n",
    "\n",
    "La deserción funciona de forma similar al arranque. Puede pensar en cada red neuronal que resulta de un conjunto diferente de neuronas que se eliminan como un miembro individual en un conjunto. A medida que avanza el entrenamiento, el programa crea más redes neuronales de esta manera. Sin embargo, el abandono no requiere la misma cantidad de procesamiento que el arranque. Las nuevas redes neuronales creadas son temporales; existen solo para una iteración de entrenamiento. El resultado final también es una única red neuronal, en lugar de un conjunto de redes neuronales para promediar juntas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# Read the data set\n",
    "df = pd.read_csv(\n",
    "    \"https://data.heatonresearch.com/data/t81-558/jh-simple-dataset.csv\",\n",
    "    na_values=['NA','?'])\n",
    "\n",
    "# Generar dummies para 'job'\n",
    "df = pd.concat([df,pd.get_dummies(df['job'],prefix=\"job\")],axis=1)\n",
    "df.drop('job', axis=1, inplace=True)\n",
    "\n",
    "# Generar dummies para 'area'\n",
    "df = pd.concat([df,pd.get_dummies(df['area'],prefix=\"area\")],axis=1)\n",
    "df.drop('area', axis=1, inplace=True)\n",
    "\n",
    "# Tratar Missing values en 'income'\n",
    "med = df['income'].median()\n",
    "df['income'] = df['income'].fillna(med)\n",
    "\n",
    "# Standarizar rangos\n",
    "df['income'] = zscore(df['income'])\n",
    "df['aspect'] = zscore(df['aspect'])\n",
    "df['save_rate'] = zscore(df['save_rate'])\n",
    "df['age'] = zscore(df['age'])\n",
    "df['subscriptions'] = zscore(df['subscriptions'])\n",
    "\n",
    "# Convertir a numpy - Clasificación\n",
    "x_columns = df.columns.drop('product').drop('id')\n",
    "x = df[x_columns].values\n",
    "dummies = pd.get_dummies(df['product']) # Clasificación\n",
    "products = dummies.columns\n",
    "y = dummies.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will see how to apply dropout to classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold #1\n",
      "Fold score (accuracy): 0.66\n",
      "Fold #2\n",
      "Fold score (accuracy): 0.7375\n",
      "Fold #3\n",
      "Fold score (accuracy): 0.735\n",
      "Fold #4\n",
      "Fold score (accuracy): 0.6975\n",
      "Fold #5\n",
      "Fold score (accuracy): 0.69\n",
      "Final score (accuracy): 0.704\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "# Classification con dropout en Keras\n",
    "########################################\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# Cross-validate\n",
    "kf = KFold(5, shuffle=True, random_state=42)\n",
    "    \n",
    "oos_y = []\n",
    "oos_pred = []\n",
    "fold = 0\n",
    "\n",
    "for train, test in kf.split(x):\n",
    "    fold+=1\n",
    "    print(f\"Fold #{fold}\")\n",
    "        \n",
    "    x_train = x[train]\n",
    "    y_train = y[train]\n",
    "    x_test = x[test]\n",
    "    y_test = y[test]\n",
    "    \n",
    "    #kernel_regularizer=regularizers.l2(0.01),\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(50, input_dim=x.shape[1], activation='relu')) # Oculta 1\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(25, activation='relu', \\\n",
    "                activity_regularizer=regularizers.l1(1e-4))) # Oculta 2\n",
    "    # Por lo general, no agregue el abandono después de la capa oculta final\n",
    "    #model.add(Dropout(0.5)) \n",
    "    model.add(Dense(y.shape[1],activation='softmax')) # Output\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "    model.fit(x_train,y_train,validation_data=(x_test,y_test),\\\n",
    "              verbose=0,epochs=500)\n",
    "    \n",
    "    pred = model.predict(x_test)\n",
    "    \n",
    "    oos_y.append(y_test)\n",
    "    # probabilidades brutas a la clase elegida (probabilidad más alta)\n",
    "    pred = np.argmax(pred,axis=1) \n",
    "    oos_pred.append(pred)        \n",
    "\n",
    "    # fold's accuracy\n",
    "    y_compare = np.argmax(y_test,axis=1) # accuracy\n",
    "    score = metrics.accuracy_score(y_compare, pred)\n",
    "    print(f\"Fold score (accuracy): {score}\")\n",
    "\n",
    "\n",
    "# Cree la lista de predicción oos y calcule el error.\n",
    "oos_y = np.concatenate(oos_y)\n",
    "oos_pred = np.concatenate(oos_pred)\n",
    "oos_y_compare = np.argmax(oos_y,axis=1) # accuracy\n",
    "\n",
    "score = metrics.accuracy_score(oos_y_compare, oos_pred)\n",
    "print(f\"Final score (accuracy): {score}\")    \n",
    "    \n",
    "# cross-validated prediction\n",
    "oos_y = pd.DataFrame(oos_y)\n",
    "oos_pred = pd.DataFrame(oos_pred)\n",
    "oosDF = pd.concat( [df, oos_y, oos_pred],axis=1 )\n",
    "#oosDF.to_csv(filename_write,index=False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "interpreter": {
   "hash": "129c28eb7371f81f3b772b69408a02d1e141a0d5c957c78df09fa0d1b2bc4f41"
  },
  "kernelspec": {
   "display_name": "Python 3.9 (tensorflow)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
