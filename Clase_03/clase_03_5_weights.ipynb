{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modulo 3: TensorFlow y Keras para Neural Networks**\n",
    "* Instructor: [Juan Maniglia](https://juanmaniglia.github.io)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 3.5: Extracción de Pesos y Cálculo Manual de Redes\n",
    "\n",
    "### \n",
    "Inicialización de peso\n",
    "\n",
    "Los pesos de una red neuronal determinan la salida de la red neuronal. El proceso de entrenamiento puede ajustar estos pesos para que la red neuronal produzca resultados útiles. La mayoría de los algoritmos de entrenamiento de redes neuronales comienzan inicializando los pesos a un estado aleatorio. Luego, el entrenamiento avanza a través de una serie de iteraciones que mejoran continuamente los pesos para producir un mejor resultado.\n",
    "\n",
    "Los pesos aleatorios de una red neuronal afectan qué tan bien se puede entrenar esa red neuronal. Si una red neuronal no se entrena, puede solucionar el problema simplemente reiniciando con un nuevo conjunto de pesos aleatorios. Sin embargo, esta solución puede resultar frustrante cuando se experimenta con la arquitectura de una red neuronal y se prueban diferentes combinaciones de capas y neuronas ocultas. Si agrega una nueva capa y el rendimiento de la red mejora, debe preguntarse si esta mejora se debió a la nueva capa o a un nuevo conjunto de pesos. Debido a esta incertidumbre, buscamos dos atributos clave en un algoritmo de inicialización de pesos:\n",
    "\n",
    "* ¿Qué tan consistentemente proporciona este algoritmo buenas ponderaciones?\n",
    "* ¿Cuánta ventaja proporcionan los pesos del algoritmo?\n",
    "\n",
    "Uno de los enfoques más comunes, pero menos efectivos, para la inicialización de ponderaciones es establecer las ponderaciones en valores aleatorios dentro de un rango específico. Los números entre -1 y +1 o -5 y +5 suelen ser la elección. Si desea asegurarse de obtener el mismo conjunto de pesos aleatorios cada vez, debe usar una semilla. La semilla especifica un conjunto de pesos aleatorios predefinidos para usar. Por ejemplo, una semilla de 1000 podría producir pesos aleatorios de 0,5, 0,75 y 0,2. Estos valores siguen siendo aleatorios; no puede predecirlos, pero siempre obtendrá estos valores cuando elija una semilla de 1000.\n",
    "No todas las semillas son creadas iguales. Un problema con la inicialización de pesos aleatorios es que los pesos aleatorios creados por algunas semillas son mucho más difíciles de entrenar que otros. De hecho, los pesos pueden ser tan malos que el entrenamiento es imposible. Si descubre que no puede entrenar una red neuronal con un conjunto de pesos en particular, debe generar un nuevo conjunto de pesos utilizando una semilla diferente.\n",
    "\n",
    "Debido a que la inicialización del peso es un problema, ha habido una investigación considerable al respecto. En este curso usamos el algoritmo de inicialización de peso de Xavier, produce buenos pesos con una consistencia razonable. Este algoritmo relativamente simple utiliza números aleatorios normalmente distribuidos.\n",
    "\n",
    "Para utilizar la inicialización del peso de Xavier, es necesario comprender que los números aleatorios normalmente distribuidos no son los típicos números aleatorios entre 0 y 1 que generan la mayoría de los lenguajes de programación. De hecho, los números aleatorios normalmente distribuidos se centran en una media ($\\mu$, mu) que normalmente es 0. Si 0 es el centro (media), entonces obtendrá un número igual de números aleatorios por encima y por debajo de 0. La siguiente pregunta es qué tan lejos se aventurarán estos números aleatorios de 0. En teoría, podría terminar con números positivos y negativos cerca de los rangos máximos positivos y negativos admitidos por su computadora. Sin embargo, la realidad es que es más probable que vea números aleatorios que están entre 0 y tres desviaciones estándar del centro.\n",
    "\n",
    "El parámetro desviación estándar ($\\sigma$, sigma) especifica el tamaño de esta desviación estándar. Por ejemplo, si especifica una desviación estándar de 10, verá principalmente números aleatorios entre -30 y +30, y los números más cercanos a 0 tienen una probabilidad mucho mayor de ser seleccionados.\n",
    "\n",
    "La figura anterior ilustra que el centro, que en este caso es 0, se generará con una probabilidad de 0,4 (40%). Además, la probabilidad disminuye muy rápidamente más allá de -2 o +2 desviaciones estándar. Al definir el centro y cuán grandes son las desviaciones estándar, puede controlar el rango de números aleatorios que recibirá.\n",
    "\n",
    "La inicialización del peso de Xavier establece todos los pesos en números aleatorios normalmente distribuidos. Estos pesos siempre están centrados en 0; sin embargo, su desviación estándar varía dependiendo de cuántas conexiones estén presentes para la capa actual de pesos. Específicamente, la Ecuación 4.2 puede determinar la desviación estándar:\n",
    "\n",
    "$$ W = \\frac{2}{n_{in}+n_{out}} $$\n",
    "\n",
    "La ecuación anterior muestra cómo obtener la varianza para todos los pesos. La raíz cuadrada de la varianza es la desviación estándar. La mayoría de los generadores de números aleatorios aceptan una desviación estándar en lugar de una varianza. Como resultado, generalmente necesita sacar la raíz cuadrada de la ecuación anterior. La Figura 3.XAVIER muestra cómo se puede inicializar una capa.\n",
    "\n",
    "**Figure 3.XAVIER: Xavier Weight Initialization**\n",
    "![Xavier Weight Initialization](images/xavier_weight.png)\n",
    "\n",
    "Este proceso se completa para cada capa en la red neuronal.\n",
    "\n",
    "### Cálculo manual de redes neuronales\n",
    "\n",
    "En esta sección, construiremos una red neuronal y la analizaremos según los pesos individuales. Entrenaremos una red neuronal simple que aprenda la función XOR. No es difícil simplemente codificar a mano las neuronas para proporcionar una [XOR function](https://en.wikipedia.org/wiki/Exclusive_or); sin embargo, para simplificar, permitiremos que Keras entrene esta red por nosotros. Solo usaremos épocas de 100000 en el optimizador de ADAM. Esto es una exageración masiva, pero obtiene el resultado, y nuestro enfoque aquí no está en la afinación. La red neuronal es pequeña. Dos entradas, dos neuronas ocultas y una única salida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cycle #1\n",
      "[[8.4662361e-08]\n",
      " [1.0000000e+00]\n",
      " [1.0000008e+00]\n",
      " [8.4662361e-08]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "import numpy as np\n",
    "\n",
    "# Creamos un dataset para la función XOR\n",
    "x = np.array([\n",
    "    [0,0],\n",
    "    [1,0],\n",
    "    [0,1],\n",
    "    [1,1]\n",
    "])\n",
    "\n",
    "y = np.array([\n",
    "    0,\n",
    "    1,\n",
    "    1,\n",
    "    0\n",
    "])\n",
    "\n",
    "# Construimos la red\n",
    "# sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "done = False\n",
    "cycle = 1\n",
    "\n",
    "while not done:\n",
    "    print(\"Cycle #{}\".format(cycle))\n",
    "    cycle+=1\n",
    "    model = Sequential()\n",
    "    model.add(Dense(2, input_dim=2, activation='relu')) \n",
    "    model.add(Dense(1)) \n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    model.fit(x,y,verbose=0,epochs=10000)\n",
    "\n",
    "    # Predict\n",
    "    pred = model.predict(x)\n",
    "    \n",
    "    # Comprobar si tiene éxito. Se necesitan varias ejecuciones con esta\n",
    "    # pequeña red\n",
    "    done = pred[0]<0.01 and pred[3]<0.01 and pred[1] > 0.9 and pred[2] > 0.9 \n",
    "    print(pred)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "interpreter": {
   "hash": "129c28eb7371f81f3b772b69408a02d1e141a0d5c957c78df09fa0d1b2bc4f41"
  },
  "kernelspec": {
   "display_name": "Python 3.9 (tensorflow)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
