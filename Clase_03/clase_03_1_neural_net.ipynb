{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modulo 3: TensorFlow y Keras para Neural Networks**\n",
    "* Instructor: [Juan Maniglia](https://juanmaniglia.github.io)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 3.1: Instroducción a Deep Learning y Neural Network\n",
    "\n",
    "Las redes neuronales fueron uno de los primeros modelos de Machine_Learning. Su popularidad ha caído dos veces y ahora está en su tercer aumento. Deep_Learning implica el uso de redes neuronales. El \"deep_\" en deep_learning se refiere a una red neuronal con muchas capas ocultas. \n",
    "\n",
    "Las redes neuronales aceptan entradas y producen salidas. La entrada a una red neuronal se denomina vector de características. El tamaño de este vector es siempre una longitud fija. Cambiar el tamaño del vector de características significa recrear toda la red neuronal. Aunque el vector de características se denomina \"vector\", no siempre es así. Un vector implica una matriz 1D. Históricamente, la entrada a una red neuronal siempre fue 1D. Sin embargo, con las redes neuronales modernas, es posible que vea datos de entrada, como:\n",
    "\n",
    "* **1D vector** - Entrada clásica a una red neuronal, similar a las filas en una hoja de cálculo. Común en el modelado predictivo.\n",
    "* **2D Matrix** - Entrada de imagen en escala de grises a una red neuronal convolucional (CNN).\n",
    "* **3D Matrix** - Entrada de imagen en color a una red neuronal convolucional (CNN).\n",
    "* **nD Matrix** - Entrada de orden superior a una CNN.\n",
    "\n",
    "Initially, this course will focus upon 1D input to neural networks.  However, later sessions will focus more heavily upon higher dimension input.\n",
    "\n",
    "**Dimensiones** El término dimensión puede resultar confuso en las redes neuronales. En el sentido de un vector de entrada 1D, la dimensión se refiere a cuántos elementos hay en esa matriz 1D. Por ejemplo, una red neuronal con diez neuronas de entrada tiene diez dimensiones. Sin embargo, ahora que tenemos CNN, la entrada también tiene dimensiones. La entrada a la red neuronal *generalmente* tendrá 1, 2 o 3 dimensiones. Cuatro o más dimensiones son inusuales. Es posible que tenga una entrada 2D a una red neuronal que tenga 64x64 píxeles. Esta configuración daría como resultado 4.096 neuronas de entrada. Esta red es 2D o 4096D, según el conjunto de dimensiones al que haga referencia.\n",
    "\n",
    "## Clasificación o Regresión\n",
    "\n",
    "Como muchos modelos, las redes neuronales pueden funcionar en clasificación o regresión:\n",
    "\n",
    "* **Regresión** - Esperas un número como predicción de tu red neuronal.\n",
    "* **Clasificación** - Espera una clase/categoría como predicción de su red neuronal.\n",
    "\n",
    "En la Figura 3.CLS-REG se muestra una red neuronal de clasificación y regresión.\n",
    "\n",
    "**Figura 3.CLS-REG muestra una red neuronal de clasificación y regresión.**\n",
    "![Neural Network Classification and Regression](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/class_2_ann_class_reg.png \"Neural Network Classification and Regression\")\n",
    "\n",
    "NObserve que la salida de la red neuronal de regresión es numérica y la salida de la clasificación es una clase. Las redes de regresión, o clasificación de dos clases, siempre tienen una única salida. Las redes neuronales de clasificación tienen una neurona de salida para cada categoría.\n",
    "\n",
    "## Neuronas y Capas\n",
    "\n",
    "La mayoría de las estructuras de redes neuronales utilizan algún tipo de neurona. Existen muchos tipos diferentes de redes neuronales, y los programadores introducen estructuras de redes neuronales experimentales todo el tiempo. En consecuencia, no es posible cubrir todas las arquitecturas de redes neuronales. Sin embargo, existen algunos puntos en común entre las implementaciones de redes neuronales. Un algoritmo que se denomina red neuronal normalmente estará compuesto por unidades individuales interconectadas, aunque estas unidades puedan o no llamarse neuronas. El nombre de una unidad de procesamiento de red neuronal varía entre las fuentes de literatura. Podría llamarse nodo, neurona o unidad.\n",
    "\n",
    "Un diagrama muestra la estructura abstracta de una sola neurona artificial en la Figura 3.ANN.\n",
    "\n",
    "**Figure 3.ANN: Una neurona artificial**\n",
    "![An Artificial Neuron](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/class_2_abstract_nn.png \"An Artificial Neuron\")\n",
    " \n",
    "La neurona artificial recibe información de una o más fuentes que pueden ser otras neuronas o datos que ingresan a la red desde un programa de computadora. Esta entrada suele ser de punto flotante o binaria. A menudo, la entrada binaria se codifica en coma flotante al representar verdadero o falso como 1 o 0. A veces, el programa también representa la entrada binaria como si usara un sistema bipolar con verdadero como uno y falso como -1.\n",
    "\n",
    "\n",
    "Una neurona artificial multiplica cada una de estas entradas por un peso. Luego suma estas multiplicaciones y pasa esta suma a una función de activación. Algunas redes neuronales no utilizan una función de activación. La siguiente ecuación resume la salida calculada de una neurona:\n",
    "\n",
    "$$ f(x,w) = \\phi(\\sum_i(\\theta_i \\cdot x_i)) $$\n",
    "\n",
    "En la ecuación anterior, las variables $x$ y $\\theta$ representan la entrada y los pesos de la neurona. La variable $i$ corresponde al número de pesos y entradas. Siempre debe tener el mismo número de pesos como entradas. La red neuronal multiplica cada peso por su entrada respectiva y alimenta los productos de estas multiplicaciones en una función de activación que se denota con la letra griega $\\phi$ (phi). Este proceso da como resultado una única salida de la neurona.  \n",
    "\n",
    "La neurona anterior tiene dos entradas más el sesgo como una tercera. Esta neurona podría aceptar el siguiente vector de características de entrada:\n",
    "\n",
    "$$ [1,2] $$\n",
    "\n",
    "Debido a que está presente una neurona de polarización, el programa debe agregar el valor de uno, de la siguiente manera:\n",
    "\n",
    "$$ [1,2,1] $$\n",
    "\n",
    "Los pesos para una capa de 3 entradas (2 entradas reales + sesgo) siempre tendrán un peso adicional, para el sesgo. Un vector de peso podría ser:\n",
    "\n",
    "$$ [ 0.1, 0.2, 0.3] $$\n",
    "\n",
    "Para calcular la suma, realice lo siguiente:\n",
    "\n",
    "$$ 0.1*1 + 0.2*2 + 0.3*1 = 0.8 $$\n",
    "\n",
    "El programa pasa un valor de 0,8 a la función $\\phi$ (phi), que representa la función de activación.\n",
    "\n",
    "La figura de arriba muestra la estructura con solo un bloque de construcción. Puede encadenar muchas neuronas artificiales para construir una red neuronal artificial (ANN). Piense en las neuronas artificiales como bloques de construcción para los cuales los círculos de entrada y salida son los conectores. La Figura 3.ANN-3 muestra una red neuronal artificial compuesta por tres neuronas:\n",
    "\n",
    "**Figure 3.ANN-3: Red neuronal de tres neuronas**\n",
    "![Three Neuron Neural Network](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/ann-simple.png \"Three Neuron Neural Network\")\n",
    "\n",
    "El diagrama anterior muestra tres neuronas interconectadas. Esta representación es esencialmente esta figura, menos unas pocas entradas, se repite tres veces y luego se conectó. Además dispone de un total de cuatro entradas y una sola salida. La salida de las neuronas **N1** y **N2** alimenta a **N3** para producir la salida O. Para calcular la salida de esta red; realizamos la ecuación anterior tres veces. Las primeras dos veces calculan ** N1 ** y ** N2 **, y el tercer cálculo utiliza la salida de ** N1 ** y ** N2 ** para calcular ** N3 **.\n",
    "   \n",
    "Los diagramas de redes neuronales no suelen mostrar el nivel de detalle que se ve en la figura anterior. Para simplificar el gráfico, podemos omitir las funciones de activación y las salidas intermedias, y este proceso da como resultado la Figura 3.SANN-3. \n",
    "\n",
    "**Figure 3.SANN-3: Red neuronal de tres neuronas**\n",
    "![Three Neuron Neural Network](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/typical-ann.png \"Three Neuron Neural Network\")\n",
    " \n",
    "Mirando la figura anterior, puede ver dos componentes adicionales de las redes neuronales. Primero, considere que el gráfico representa las entradas y salidas como círculos abstractos de líneas punteadas. La entrada y la salida podrían ser partes de una red neuronal más extensa. Sin embargo, la entrada y la salida suelen ser un tipo particular de neurona que acepta datos del programa de computadora usando la red neuronal, y las neuronas de salida devuelven un resultado al programa. Este tipo de neurona se llama neurona de entrada. Discutiremos estas neuronas en la siguiente sección. Esta figura muestra las neuronas dispuestas en capas. Las neuronas de entrada son la primera capa, las neuronas **N1** y **N2** crean la segunda capa, la tercera capa contiene **N3** y la cuarta capa tiene O. La mayoría de las redes neuronales organizan las neuronas en capas .\n",
    "\n",
    "Las neuronas que forman una capa comparten varias características. Primero, cada neurona en una capa tiene la misma función de activación. Sin embargo, las funciones de activación empleadas en cada capa pueden ser diferentes. Cada una de las capas se conecta completamente con la siguiente capa. En otras palabras, cada neurona de una capa tiene una conexión con las neuronas de la capa anterior. La primera figura no está completamente conectada. Faltan conexiones en varias capas. Por ejemplo, **I1** y **N2** no se conectan. La siguiente red neuronal en la Figura 3.F-ANN está completamente conectada y tiene una capa adicional.\n",
    "\n",
    "**Figure 3.F-ANN: Diagrama de red neuronal completamente conectado**\n",
    "![Fully Connected Neural Network Diagram](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/ann-dense.png \"Fully Connected Neural Network Diagram\")\n",
    "\n",
    "En esta figura, se ve una red neuronal multicapa totalmente conectada. Las redes, como esta, siempre tendrán una capa de entrada y otra de salida. La estructura de capa oculta determina el nombre de la arquitectura de red. La red de esta figura es una red de dos capas ocultas. La mayoría de las redes tendrán entre cero y dos capas ocultas. A menos que haya implementado estrategias de aprendizaje profundo, las redes con más de dos capas ocultas son raras.\n",
    "\n",
    "También puede notar que las flechas siempre apuntan hacia abajo o hacia adelante desde la entrada hasta la salida. Este tipo de red neuronal se denomina red neuronal feedforward. Más adelante en este curso, veremos redes neuronales recurrentes que forman bucles invertidos entre las neuronas.\n",
    "\n",
    "## tipos de neuronas\n",
    "\n",
    "En la última sección, presentamos brevemente la idea de que existen diferentes tipos de neuronas. Ahora explicaremos todos los tipos de neuronas descritos en el curso. No todas las redes neuronales utilizarán todos los tipos de neuronas. También es posible que una sola neurona desempeñe el papel de varios tipos de neuronas diferentes.\n",
    "\n",
    "Por lo general, hay cuatro tipos de neuronas en una red neuronal:\n",
    "\n",
    "* **Neuronas de entrada**: cada neurona de entrada se asigna a un elemento en el vector de características.\n",
    "* **Neuronas ocultas**: las neuronas ocultas permiten que la red neuronal sea abstracta y procese la entrada en la salida.\n",
    "* **Neuronas de salida**: cada neurona de salida calcula una parte de la salida.\n",
    "* **Neuronas de contexto**: mantiene el estado entre llamadas a la red neuronal para predecir.\n",
    "* **Neuronas polarizadas**: funcionan de forma similar a la intersección con el eje y de una ecuación lineal.\n",
    "\n",
    "Estas neuronas se agrupan en capas:\n",
    "\n",
    "* **Capa de entrada**: la capa de entrada acepta vectores de características del conjunto de datos. Las capas de entrada suelen tener una neurona de polarización.\n",
    "* **Capa de salida**: la salida de la red neuronal. La capa de salida no tiene una neurona de polarización.\n",
    "* **Capas ocultas**: capas que se encuentran entre las capas de entrada y salida. Cada capa oculta suele tener una neurona de polarización.\n",
    "\n",
    "\n",
    "### Neuronas de entrada y salida\n",
    "\n",
    "Casi todas las redes neuronales tienen neuronas de entrada y salida. Las neuronas de entrada aceptan datos del programa para la red. La neurona de salida proporciona datos procesados ​​de la red al programa. El programa agrupará estas neuronas de entrada y salida en capas separadas llamadas capa de entrada y salida. El programa normalmente representa la entrada a una red neuronal como una matriz o vector. El número de elementos contenidos en el vector debe ser igual al número de neuronas de entrada. Por ejemplo, una red neuronal con tres neuronas de entrada podría aceptar el siguiente vector de entrada:\n",
    "\n",
    "$$ [0.5, 0.75, 0.2] $$\n",
    "\n",
    "Las redes neuronales suelen aceptar vectores de coma flotante como entrada. Del mismo modo, las redes neuronales generarán un vector con una longitud igual al número de neuronas de salida. La salida será a menudo un solo valor de una sola neurona de salida. Para ser coherentes, representaremos la salida de una red de neuronas de salida única como un vector de un solo elemento.\n",
    "\n",
    "Observe que las neuronas de entrada no tienen funciones de activación. Como se demostró anteriormente, las neuronas de entrada son poco más que marcadores de posición. La entrada simplemente se pondera y se suma. Además, el tamaño de los vectores de entrada y salida de la red neuronal será el mismo si la red neuronal tiene neuronas que son tanto de entrada como de salida.\n",
    "\n",
    "### Neuronas ocultas\n",
    "\n",
    "Las neuronas ocultas tienen dos características esenciales. Primero, las neuronas ocultas solo reciben información de otras neuronas, como la entrada u otras neuronas ocultas. En segundo lugar, las neuronas ocultas solo emiten a otras neuronas, como la salida u otras neuronas ocultas. Las neuronas ocultas ayudan a la red neuronal a comprender la entrada y forman la salida. Sin embargo, estas capas ocultas no procesan directamente los datos entrantes o la salida final. Los programadores suelen agrupar neuronas ocultas en capas ocultas totalmente conectadas.\n",
    "\n",
    "Una pregunta común para los programadores se refiere a la cantidad de neuronas ocultas en una red. Dado que la respuesta a esta pregunta es compleja, más de una sección del curso incluirá una discusión relevante sobre la cantidad de neuronas ocultas. Antes del aprendizaje profundo, los investigadores generalmente sugerían que algo más que una sola capa oculta era excesivo (Hornik, 1991). Los investigadores han demostrado que una red neuronal de una sola capa oculta puede funcionar como un aproximador universal. En otras palabras, esta red debería poder aprender a producir (o aproximar) cualquier salida a partir de cualquier entrada siempre que tenga suficientes neuronas ocultas en una sola capa.\n",
    "\n",
    "Otra razón por la que los investigadores solían burlarse de la idea de capas ocultas adicionales es que estas capas impedirían el entrenamiento de la red neuronal. El entrenamiento se refiere al proceso que determina buenos valores de peso. Antes de que los investigadores introdujeran técnicas de aprendizaje profundo, simplemente no teníamos una forma eficiente de entrenar una red profunda, que son redes neuronales con una gran cantidad de capas ocultas. Aunque una red neuronal de una sola capa oculta teóricamente puede aprender cualquier cosa, el aprendizaje profundo facilita una representación más compleja de patrones en los datos. \n",
    "\n",
    "### Neuronas de polarización\n",
    "\n",
    "Los programadores agregan neuronas sesgadas a las redes neuronales para ayudarlos a aprender patrones. Las neuronas de polarización funcionan como una neurona de entrada que siempre produce un valor de 1. Debido a que las neuronas de polarización tienen una salida constante de 1, no están conectadas a la capa anterior. El valor de 1, que se denomina activación de polarización, se puede establecer en valores distintos de 1. Sin embargo, 1 es la activación de polarización más común. No todas las redes neuronales tienen neuronas sesgadas. La Figura 3. BIAS muestra una red neuronal de una sola capa oculta con neuronas de polarización:\n",
    "\n",
    "**Figure 3.BIAS: Red neuronal con neuronas de polarización**\n",
    "![Neural Network with Bias Neurons](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/class_2_ann.png \"Neural Network with Bias Neurons\")\n",
    " \n",
    "La red anterior contiene tres neuronas de polarización. Cada nivel, excepto la capa de salida, incluye una única neurona de polarización. Las neuronas de polarización permiten que el programa cambie la salida de una función de activación. Veremos precisamente cómo ocurre este cambio más adelante en el módulo cuando discutamos las funciones de activación.  \n",
    "\n",
    "### Neuronas de contexto\n",
    "\n",
    "Las redes neuronales recurrentes utilizan neuronas de contexto para mantener el estado. Este tipo de neurona permite que la red neuronal mantenga el estado. Como resultado, es posible que una entrada dada no siempre produzca la misma salida. Esta inconsistencia es similar al funcionamiento de los cerebros biológicos. Considere cómo el contexto influye en su respuesta cuando escucha una bocina fuerte. Si escucha el ruido mientras cruza la calle, es posible que se sobresalte, deje de caminar y mire en dirección a la bocina. Si escuchas la bocina mientras ves una película de acción y aventuras en una sala de cine, no respondes de la misma manera. Por lo tanto, las entradas anteriores le brindan el contexto para procesar la entrada de audio de una bocina.\n",
    "\n",
    "Las series de tiempo son una aplicación de las neuronas de contexto. Es posible que deba entrenar una red neuronal para aprender señales de entrada para realizar el reconocimiento de voz o para predecir tendencias en los precios de los valores. Las neuronas de contexto son una forma en que las redes neuronales manejan datos de series temporales. La Figura 3.CTX muestra cómo una red neuronal podría organizar las neuronas de contexto:\n",
    "\n",
    "**Figure 3.CTX: Red neuronal con neuronas de contexto**\n",
    "![Neural Network with Context Neurons](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/ann-context.png \"Neural Network with Context Neurons\")\n",
    " \n",
    "Esta red neuronal tiene una sola neurona de entrada y salida. Entre las capas de entrada y salida hay dos neuronas ocultas y dos neuronas de contexto. Aparte de las dos neuronas de contexto, esta red es la misma que las redes demostradas anteriormente.\n",
    "\n",
    "Cada neurona de contexto tiene un valor que comienza en 0 y siempre recibe una copia del oculto uno o del oculto dos del uso anterior de la red. Las dos líneas discontinuas en esta figura significan que la neurona de contexto es una copia directa sin otra ponderación. Las otras líneas indican que la salida está ponderada por uno de los seis valores de peso enumerados anteriormente. El cálculo de la salida todavía ocurre de la misma manera. El valor de la neurona de salida sería la suma de las cuatro entradas, multiplicado por sus pesos y aplicado a la función de activación.\n",
    "\n",
    "### Otros tipos de neuronas\n",
    "\n",
    "Las unidades individuales que componen una red neuronal no siempre se denominan neuronas. Los investigadores a veces se refieren a estas neuronas como nodos, unidades o sumas. En módulos posteriores de este curso, exploraremos el aprendizaje profundo que utiliza máquinas de Boltzmann para desempeñar el papel de las neuronas. Casi siempre construirá redes neuronales de conexiones ponderadas entre estas unidades.\n",
    "\n",
    "## ¿Por qué son necesarias las neuronas de polarización?\n",
    "\n",
    "Las funciones de activación vistas en la sección anterior especifican la salida de una sola neurona. Juntos, el peso y el sesgo de una neurona dan forma a la salida de la activación para producir la salida deseada. Para ver cómo ocurre este proceso, considere la siguiente ecuación. Representa una red neuronal de activación sigmoidea de entrada única.\n",
    "\n",
    "$$ f(x,w,b) = \\frac{1}{1 + e^{-(wx+b)}} $$ \n",
    "\n",
    "La variable $x$ representa la única entrada a la red neuronal. Las variables $w$ y $b$ especifican el peso y el sesgo de la red neuronal. La ecuación anterior es una combinación de la suma ponderada de las entradas y la función de activación sigmoidea. Para esta sección, consideraremos la función sigmoidea porque demuestra el efecto que tiene una neurona de polarización.\n",
    "\n",
    "Los pesos de la neurona te permiten ajustar la pendiente o forma de la función de activación. La figura 3.A-WEIGHT muestra el efecto sobre la salida de la función de activación sigmoidea si se varía el peso:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Figure 3.A-WEIGHT: Cambio de peso de la neurona**\n",
    "![Adjusting Weight](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/class_2_bias_weight.png \"Neuron Weight Shifting\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El diagrama anterior muestra varias curvas sigmoideas usando los siguientes parámetros:\n",
    "\n",
    "\n",
    "$$ \\\\\n",
    "f(x,0.5,0.0) \\\\\n",
    "f(x,1.0,0.0) \\\\\n",
    "f(x,1.5,0.0) \\\\\n",
    "f(x,2.0,0.0)\n",
    "$$\n",
    "\n",
    "\n",
    "Para producir las curvas, no usamos sesgo, que es evidente en el tercer parámetro de 0 en cada caso. El uso de cuatro valores de peso produce cuatro curvas sigmoideas diferentes en la figura anterior. No importa el peso, siempre obtenemos el mismo valor de 0,5 cuando *x* es 0 porque todas las curvas alcanzan el mismo punto cuando x es 0. Es posible que necesitemos que la red neuronal produzca otros valores cuando la entrada está cerca de 0,5.\n",
    "\n",
    "El sesgo cambia la curva sigmoidea, lo que permite valores distintos de 0,5 cuando x está cerca de 0. La Figura 3.A-BIAS muestra el efecto de usar un peso de 1,0 con varios sesgos diferentes:\n",
    "\n",
    "**Figure 3.A-BIAS: Cambio de polarización neuronal**\n",
    "![Adjusting Bias](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/class_2_bias_value.png \"Neuron Bias Shifting\")\n",
    "\n",
    "El diagrama anterior muestra varias curvas sigmoideas con los siguientes parámetros:\n",
    "\n",
    "$$ \\\\\n",
    "f(x,1.0,1.0) \\\\\n",
    "f(x,1.0,0.5) \\\\\n",
    "f(x,1.0,1.5) \\\\\n",
    "f(x,1.0,2.0)\n",
    "$$\n",
    "\n",
    "Usamos un peso de 1.0 para estas curvas en todos los casos. Cuando utilizamos varios sesgos diferentes, las curvas sigmoideas se desplazaron hacia la izquierda o hacia la derecha. Debido a que todas las curvas se fusionan en la parte superior derecha o inferior izquierda, no es un cambio completo.\n",
    "\n",
    "Cuando juntamos el sesgo y los pesos, produjeron una curva que creó la salida necesaria de una neurona. Las curvas anteriores son la salida de una sola neurona. En una red completa, la salida de muchas neuronas diferentes se combinará para producir patrones de salida intrincados.\n",
    "\n",
    "\n",
    "## Funciones de activación modernas\n",
    "\n",
    "Las funciones de activación, también conocidas como funciones de transferencia, se utilizan para calcular la salida de cada capa de una red neuronal. Históricamente, las redes neuronales han utilizado una función de activación tangente hiperbólica, sigmoidea/logística o lineal. Sin embargo, las redes neuronales profundas modernas utilizan principalmente las siguientes funciones de activación:\n",
    "\n",
    "* **Rectified Linear Unit (ReLU)** - Se utiliza para la salida de capas ocultas.. [[Cite:glorot2011deep]](http://proceedings.mlr.press/v15/glorot11a/glorot11a.pdf)\n",
    "* **Softmax** -Se utiliza para la salida de redes neuronales de clasificación.. [Softmax Example](http://www.heatonresearch.com/aifh/vol3/softmax.html)\n",
    "* **Linear** - Se utiliza para la salida de redes neuronales de regresión (o clasificación de 2 clases).\n",
    "\n",
    "### Función de activación lineal\n",
    "La función de activación más básica es la función lineal porque no cambia en absoluto la salida de la neurona. La siguiente ecuación 1.2 muestra cómo el programa implementa típicamente una función de activación lineal:\n",
    "\n",
    "$$ \\phi(x) = x $$\n",
    "\n",
    "Como puede observar, esta función de activación simplemente devuelve el valor que le pasó la neurona. La figura 3.LIN muestra el gráfico de una función de activación lineal:\n",
    "\n",
    "**Figure 3.LIN: Función de activación lineal**\n",
    "![Linear Activation Function](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/graphs-linear.png \"Linear Activation Function\")\n",
    "\n",
    " \n",
    "Las redes neuronales de regresión, aquellas que aprenden a proporcionar valores numéricos, generalmente usarán una función de activación lineal en su capa de salida. Las redes neuronales de clasificación, aquellas que determinan una clase apropiada para su entrada, a menudo utilizarán una función de activación softmax para su capa de salida.\n",
    "\n",
    "### Rectified Linear Units (ReLU)\n",
    "\n",
    "Introducida en 2000 por Teh & Hinton, la unidad lineal rectificada (ReLU) ha tenido una adopción muy rápida en los últimos años. Antes de la función de activación ReLU, los programadores generalmente consideraban la tangente hiperbólica como la función de activación preferida. La mayoría de las investigaciones actuales ahora recomiendan el ReLU debido a los resultados de entrenamiento superiores. Como resultado, la mayoría de las redes neuronales deberían utilizar ReLU en capas ocultas y softmax o linear en la capa de salida. La siguiente ecuación muestra la función ReLU directa:\n",
    "\n",
    "$$ \\phi(x) = \\max(0, x) $$\n",
    "\n",
    "Ahora examinaremos por qué ReLU suele funcionar mejor que otras funciones de activación para capas ocultas. Parte del mayor rendimiento se debe a que la función de activación de ReLU es una función lineal, no saturada. A diferencia de las funciones de activación sigmoide/logística o tangente hiperbólica, ReLU no se satura a -1, 0 o 1. Una función de activación de saturación se mueve hacia un valor y eventualmente lo alcanza. La función tangente hiperbólica, por ejemplo, se satura a -1 cuando x disminuye y uno cuando x aumenta. La Figura 3.RELU muestra el gráfico de la función de activación de ReLU:\n",
    "\n",
    "**Figure 3.RELU: Rectified Linear Units (ReLU)**\n",
    "![Rectified Linear Units (ReLU)](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/graphs-relu.png \"Rectified Linear Units (ReLU)\")\n",
    " \n",
    "La mayoría de las investigaciones actuales afirman que las capas ocultas de su red neuronal deberían usar la activación de ReLU.\n",
    "\n",
    "### Función de Activación Softmax\n",
    "\n",
    "La función de activación final que examinaremos es la función de activación softmax. Junto con la función de activación lineal, generalmente puede encontrar la función softmax en la capa de salida de una red neuronal. Las redes neuronales de clasificación suelen emplear la función softmax. La neurona que tiene el valor más alto reclama la entrada como miembro de su clase. Debido a que es un método preferible, la función de activación softmax obliga a la salida de la red neuronal a representar la probabilidad de que la entrada caiga en cada una de las clases. Sin el softmax, las salidas de la neurona son simplemente valores numéricos, y el más alto indica la clase ganadora.\n",
    "\n",
    "Para ver cómo el programa usa la función de activación softmax, veremos un problema típico de clasificación de redes neuronales. El conjunto de datos del iris contiene cuatro medidas para 150 flores de iris diferentes. Cada una de estas flores pertenece a una de las tres especies de iris. Cuando proporciona las medidas de una flor, la función softmax permite que la red neuronal le proporcione la probabilidad de que estas medidas pertenezcan a cada una de las tres especies. Por ejemplo, la red neuronal podría decirle que hay un 80 % de probabilidad de que el iris sea setosa, un 15 % de probabilidad de que sea virginica y solo un 5 % de probabilidad de que sea versicolor. Debido a que estas son probabilidades, deben sumar 100%. No podría haber un 80 % de probabilidad de setosa, un 75 % de probabilidad de virginica y un 20 % de probabilidad de versicolor; este tipo de resultado no tendría sentido.\n",
    "\n",
    "Para clasificar los datos de entrada en una de las tres especies de iris, necesitará una neurona de salida para cada una de las tres especies. Las neuronas de salida no especifican inherentemente la probabilidad de cada una de las tres especies. Por lo tanto, es deseable proporcionar probabilidades que sumen 100%. La red neuronal te dirá la probabilidad de que una flor sea cada una de las tres especies. Para obtener la probabilidad, use la función softmax en la siguiente ecuación:\n",
    "\n",
    "$$ \\phi_i(x) = \\frac{exp(x_i)}{\\sum_{j}^{ }exp(x_j))} $$\n",
    "\n",
    "En la ecuación anterior, $i$ representa el índice de la neurona de salida ($o$) que el programa está calculando y $j$ representa los índices de todas las neuronas en el grupo/nivel. La variable z designa la matriz de neuronas de salida. Es importante tener en cuenta que el programa calcula la activación de softmax de manera diferente a las otras funciones de activación en este módulo. Cuando softmax es la función de activación, la salida de una sola neurona depende de las otras neuronas de salida.\n",
    " \n",
    "## Funciones de activación clásicas\n",
    "\n",
    "### Función de activación de pasos\n",
    "\n",
    "The step or threshold activation function is another simple activation function.  Neural networks were originally called perceptrons.\n",
    "\n",
    "$$ \\phi(x)=\\begin{cases}\n",
    "    1, \\text{if x >= 0.5}.\\\\\n",
    "    0, \\text{otherwise}.\n",
    "  \\end{cases}\n",
    "$$\n",
    "  \n",
    "Esta ecuación genera un valor de 1,0 para valores entrantes de 0,5 o superiores y 0 para todos los demás valores. Las funciones de paso, también conocidas como funciones de umbral, solo devuelven 1 (verdadero) para valores que están por encima del umbral especificado, como se ve en la Figura 3.\n",
    "\n",
    "**Figure 3. Step Función de activación**\n",
    "![Step Activation Function](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/graphs-step.png \"Step Activation Function\")\n",
    " \n",
    "### Función de activación sigmoidea\n",
    "\n",
    "La función de activación sigmoidea o logística es una opción muy común para las redes neuronales de avance que necesitan generar solo números positivos. A pesar de su uso generalizado, la tangente hiperbólica o la función de activación de la unidad lineal rectificada (ReLU) suele ser una opción más adecuada. Presentamos la función de activación de ReLU más adelante en este módulo. La siguiente ecuación muestra la función de activación sigmoidea:\n",
    "\n",
    "$$ \\phi(x) = \\frac{1}{1 + e^{-x}} $$\n",
    "\n",
    "Use la función sigmoide para asegurarse de que los valores permanezcan dentro de un rango relativamente pequeño, como se ve en la Figura 3.SIGMOID:\n",
    "\n",
    "**Figure 3.SIGMOID: Función de activación sigmoidea**\n",
    "![Sigmoid Activation Function](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/graphs-sigmoid.png \"Sigmoid Activation Function\")\n",
    "\n",
    "Como puede ver en el gráfico anterior, los valores se pueden forzar a un rango. Aquí, la función comprimía los valores por encima o por debajo de 0 al rango aproximado entre 0 y 1.\n",
    "\n",
    "### Función de activación de tangente hiperbólica\n",
    "\n",
    "La función de tangente hiperbólica también es una función de activación predominante para redes neuronales que deben generar valores en el rango entre -1 y 1. Esta función de activación es simplemente la función de tangente hiperbólica (tanh), como se muestra en la siguiente ecuación:\n",
    "\n",
    "$$ \\phi(x) = \\tanh(x) $$\n",
    "\n",
    "La gráfica de la función tangente hiperbólica tiene una forma similar a la función de activación sigmoidea, como se ve en la Figura 3.HTAN.\n",
    "\n",
    "**Figure 3.HTAN: Función de activación de tangente hiperbólica**\n",
    "![Hyperbolic Tangent Activation Function](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/graphs-tanh.png \"Hyperbolic Tangent Activation Function\")\n",
    " \n",
    "TLa función tangente hiperbólica tiene varias ventajas sobre la función de activación sigmoidea.  "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3.9 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
